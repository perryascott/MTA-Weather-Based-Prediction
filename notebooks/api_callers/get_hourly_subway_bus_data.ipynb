{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import pandas as pd\n",
    "from sodapy import Socrata\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "from backoff import on_exception, expo\n",
    "\n",
    "# Load environment variables from ../.api_key\n",
    "load_dotenv(dotenv_path='../.api_key')\n",
    "\n",
    "# Get the MTA_API_KEY from environment variables\n",
    "mta_api_key = os.getenv('MTA_API_KEY')\n",
    "\n",
    "# Check if the API key was loaded\n",
    "if not mta_api_key:\n",
    "    raise ValueError(\"MTA_API_KEY not found in environment variables. Please check your .api_key file.\")\n",
    "\n",
    "\n",
    "# Initialize the Socrata client with the API key\n",
    "client = Socrata(\"data.ny.gov\", mta_api_key, timeout=60)\n",
    "\n",
    "# Dataset identifier for MTA Subway Data\n",
    "dataset_identifier = \"wujg-7c2s\"\n",
    "\n",
    "# Dataset identifier for MTA Bus Data\n",
    "# dataset_identifier = \"kv7t-n8in\"\n",
    "\n",
    "# Filter data from January 1st, 2022 onward using the correct date field\n",
    "where_clause = \"transit_timestamp >= '2020-01-01T00:00:00'\"\n",
    "\n",
    "# Set the number of records to fetch per batch\n",
    "limit = 50000  # Fetch 50,000 rows per batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import pandas as pd\n",
    "from sodapy import Socrata\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "\n",
    "# Load environment variables from ../.api_key\n",
    "load_dotenv(dotenv_path='../.api_key')\n",
    "\n",
    "# Get the MTA_API_KEY from environment variables\n",
    "mta_api_key = os.getenv('MTA_API_KEY')\n",
    "\n",
    "# Check if the API key was loaded\n",
    "if not mta_api_key:\n",
    "    raise ValueError(\"MTA_API_KEY not found in environment variables. Please check your .api_key file.\")\n",
    "\n",
    "# Initialize the Socrata client with the API key\n",
    "client = Socrata(\"data.ny.gov\", mta_api_key, timeout=60)\n",
    "\n",
    "# Set the date range for data fetching\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2024, 10, 24)  # Adjust as needed\n",
    "\n",
    "# Time delta for each batch (e.g., one day)\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "# State management\n",
    "state_file = 'fetch_state.json'\n",
    "\n",
    "if os.path.exists(state_file):\n",
    "    # Load state from file\n",
    "    with open(state_file, 'r') as f:\n",
    "        state = json.load(f)\n",
    "        current_date_str = state.get('current_date')\n",
    "        if current_date_str:\n",
    "            current_date = datetime.fromisoformat(current_date_str)\n",
    "        else:\n",
    "            current_date = start_date\n",
    "    print(f\"Resuming from date {current_date.date()}\")\n",
    "else:\n",
    "    # Initialize state if no state file exists\n",
    "    current_date = start_date\n",
    "    state = {}\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = 'mta_subway_data'\n",
    "# output_dir = 'mta_bus_data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "while current_date <= end_date:\n",
    "    max_retries = 3\n",
    "    retry_count = 0\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            next_date = current_date + delta\n",
    "\n",
    "            # Format dates for the WHERE clause\n",
    "            current_date_str = current_date.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "            next_date_str = next_date.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "            where_clause = f\"transit_timestamp >= '{current_date_str}' AND transit_timestamp < '{next_date_str}'\"\n",
    "\n",
    "            # Fetch data for the current date range\n",
    "            results = client.get(\n",
    "                dataset_identifier,\n",
    "                where=where_clause,\n",
    "                limit=1000000  # Increase limit as needed\n",
    "            )\n",
    "\n",
    "            if not results:\n",
    "                print(f\"No data returned for date {current_date.date()}.\")\n",
    "            else:\n",
    "                results_df = pd.DataFrame.from_records(results)\n",
    "\n",
    "                # Write to CSV\n",
    "                if not results_df.empty:\n",
    "                    # Create a filename for the current date\n",
    "                    output_file = os.path.join(output_dir, f'{output_dir}_{current_date.date()}.csv')\n",
    "                    \n",
    "                    # Write the data to the file\n",
    "                    results_df.to_csv(output_file, index=False)\n",
    "                    print(f\"Data for {current_date.date()} saved to {output_file}\")\n",
    "\n",
    "            # Update current_date for the next iteration\n",
    "            current_date = next_date\n",
    "\n",
    "            # Update state\n",
    "            state['current_date'] = current_date.isoformat()\n",
    "\n",
    "            # Save state to file\n",
    "            with open(state_file, 'w') as f:\n",
    "                json.dump(state, f)\n",
    "\n",
    "            # Optional: Sleep to respect API rate limits\n",
    "            time.sleep(1)\n",
    "\n",
    "            # If successful, break out of the retry loop\n",
    "            break\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            retry_count += 1\n",
    "            if retry_count < max_retries:\n",
    "                print(f\"Timeout error occurred. Retrying in 60 seconds... (Attempt {retry_count} of {max_retries})\")\n",
    "                time.sleep(60)\n",
    "            else:\n",
    "                print(f\"Max retries reached. Stopping the process.\")\n",
    "                # Close the Socrata client connection before exiting\n",
    "                client.close()\n",
    "                exit(1)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            # Close the Socrata client connection before exiting\n",
    "            client.close()\n",
    "            exit(1)\n",
    "\n",
    "# Close the Socrata client connection\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "def process_daily_csvs_for_hourly_ridership(input_dir):\n",
    "    hourly_ridership = {}\n",
    "    \n",
    "    # Get all CSV files in the input directory\n",
    "    csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
    "\n",
    "    # Sort the csv_files list by the file name in ascending order\n",
    "    csv_files.sort()\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        print(f\"Processing {csv_file}\")\n",
    "\n",
    "        file_path = os.path.join(input_dir, csv_file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert timestamp to datetime\n",
    "        df['datetime'] = pd.to_datetime(df['transit_timestamp'])\n",
    "        \n",
    "        # Localize to UTC first, then convert to Eastern Time\n",
    "        df['datetime'] = df['datetime'].dt.tz_localize('UTC', ambiguous='NaT').dt.tz_convert('US/Eastern')\n",
    "        \n",
    "        # Create hour key (YYYY-MM-DD HH:00:00)\n",
    "        df['hour_key'] = df['datetime'].dt.floor('h', ambiguous='NaT')\n",
    "        \n",
    "        # Group by hour and sum ridership\n",
    "        hourly_sum = df.groupby('hour_key')['ridership'].sum()\n",
    "        \n",
    "        # Update the hourly_ridership dictionary\n",
    "        for hour, ridership in hourly_sum.items():\n",
    "            if 0 <= ridership <= 1000000:  # Assuming max 1,000,000 riders per hour as a sanity check\n",
    "                hourly_ridership[hour] = hourly_ridership.get(hour, 0) + ridership\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_hourly = pd.DataFrame.from_dict(hourly_ridership, orient='index', columns=['total_ridership'])\n",
    "    df_hourly.index.name = 'hour'\n",
    "    df_hourly = df_hourly.sort_index()\n",
    "    \n",
    "    return df_hourly\n",
    "\n",
    "# Process the CSV files and get hourly ridership\n",
    "print(\"Processing CSV files for hourly ridership:\")\n",
    "hourly_ridership_df = process_daily_csvs_for_hourly_ridership(output_dir)\n",
    "\n",
    "print(\"Hourly ridership calculation complete.\")\n",
    "print(f\"Total hours processed: {len(hourly_ridership_df)}\")\n",
    "\n",
    "# Display the first few rows of the result\n",
    "print(\"\\nFirst few rows of hourly ridership:\")\n",
    "print(hourly_ridership_df.head())\n",
    "\n",
    "# Save the result to a CSV file\n",
    "output_file = 'hourly_bus_ridership.csv'\n",
    "hourly_ridership_df.to_csv(output_file)\n",
    "print(f\"\\nHourly ridership data saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
